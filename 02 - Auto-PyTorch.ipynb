{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from autoPyTorch import AutoNetMultilabel\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/opt/input'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BASE_PATH = os.environ.get('TRAINML_DATA_PATH') if os.environ.get('TRAINML_DATA_PATH') else '../input/lish-moa'\n",
    "BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = pd.read_csv(f'{BASE_PATH}/train_features.csv')\n",
    "train_targets = pd.read_csv(f'{BASE_PATH}/train_targets_scored.csv')\n",
    "test_features = pd.read_csv(f'{BASE_PATH}/test_features.csv')\n",
    "\n",
    "sample_submission = pd.read_csv(f'{BASE_PATH}/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    df = df.copy()\n",
    "    df.loc[:, 'cp_type'] = df.loc[:, 'cp_type'].map({'trt_cp': 0, 'ctl_vehicle': 1})\n",
    "    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    del df['sig_id']\n",
    "    return df\n",
    "\n",
    "train_data = preprocess(train_features)\n",
    "test_data = preprocess(test_features)\n",
    "\n",
    "del train_targets['sig_id']\n",
    "\n",
    "train_targets = train_targets.loc[train_data['cp_type']==0].reset_index(drop=True)\n",
    "train_data = train_data.loc[train_data['cp_type']==0].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation\n",
    "def get_tail_labels(df: pd.DataFrame, ql=[0.03, 1.]) -> list:\n",
    "    \" Find the underepresented targets a.k.a. minority labels. \"\n",
    "    irlbl = df.sum(axis=0)\n",
    "    irlbl = irlbl[(irlbl > irlbl.quantile(ql[0])) & ((irlbl < irlbl.quantile(ql[1])))]  # Filtering\n",
    "    irlbl = irlbl.max() / irlbl\n",
    "    threshold_irlbl = irlbl.median()\n",
    "    tail_labels = irlbl[irlbl > threshold_irlbl].index.tolist()\n",
    "    return tail_labels\n",
    "\n",
    "def get_minority_samples(X: pd.DataFrame, y: pd.DataFrame, ql=[0.03, 1.]):\n",
    "    \" Find minority samples associated with minority labels. \"\n",
    "    tail_labels = get_tail_labels(y, ql=ql)\n",
    "    index = y[y[tail_labels].apply(lambda x: (x == 1).any(), axis=1)].index.tolist()\n",
    "    \n",
    "    X_sub = X[X.index.isin(index)].reset_index(drop = True)\n",
    "    y_sub = y[y.index.isin(index)].reset_index(drop = True)\n",
    "    return X_sub, y_sub\n",
    "\n",
    "def nearest_neighbour(X: pd.DataFrame, neigh) -> list:\n",
    "    \" Find nearest neighbors for each sample in X dataframe. \"\n",
    "    nbs = NearestNeighbors(n_neighbors=neigh, metric='euclidean', algorithm='kd_tree').fit(X)\n",
    "    euclidean, indices = nbs.kneighbors(X)\n",
    "    return indices\n",
    "\n",
    "def MLSMOTE(X, y, n_samples, n_neighbors=5):\n",
    "    \" Generate new samples using MLSMOTE algorithm. \"\n",
    "    indices2 = nearest_neighbour(X, neigh=n_neighbors)\n",
    "    n = len(indices2)\n",
    "    new_X = np.zeros((n_samples, X.shape[1]))\n",
    "    target = np.zeros((n_samples, y.shape[1]))\n",
    "    for i in range(n_samples):\n",
    "        reference = random.randint(0, n-1)\n",
    "        neighbor = random.choice(indices2[reference, 1:])\n",
    "        all_point = indices2[reference]\n",
    "        nn_df = y[y.index.isin(all_point)]\n",
    "        ser = nn_df.sum(axis = 0, skipna = True)\n",
    "        target[i] = np.array([1 if val > 0 else 0 for val in ser])\n",
    "        ratio = random.random()\n",
    "        gap = X.loc[reference,:] - X.loc[neighbor,:]\n",
    "        new_X[i] = np.array(X.loc[reference,:] + ratio * gap)\n",
    "    new_X = pd.DataFrame(new_X, columns=X.columns)\n",
    "    target = pd.DataFrame(target, columns=y.columns)\n",
    "    return new_X, target\n",
    "\n",
    "def augment_data(X, y, oversample_args: tuple):\n",
    "    \" Augment feature/targets data (just doing oversampling for now)\"\n",
    "    n_samples, n_neighbors = oversample_args\n",
    "\n",
    "    X_sub, y_sub = get_minority_samples(X, y)\n",
    "    X_res, y_res = MLSMOTE(X_sub, y_sub, n_samples, n_neighbors)\n",
    "    X_augmented = pd.concat([X, X_res])\n",
    "    y_augmented = pd.concat([y, y_res])\n",
    "    return X_augmented, y_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_oversample_args = (1000, 5)\n",
    "train_data_augmented, train_targets_augmented = augment_data(train_data, train_targets, data_oversample_args)\n",
    "\n",
    "X = train_data_augmented.values\n",
    "Y = train_targets_augmented.values\n",
    "X_test = test_data.values\n",
    "\n",
    "X_original = train_data.values\n",
    "Y_original = train_targets.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'embeddings': ['none'],\n",
       " 'lr_scheduler': ['cosine_annealing', 'plateau'],\n",
       " 'networks': ['mlpnet', 'shapedmlpnet', 'resnet', 'shapedresnet'],\n",
       " 'preprocessors': ['none', 'truncated_svd', 'power_transformer'],\n",
       " 'result_logger_dir': 'logs/',\n",
       " 'budget_type': 'epochs',\n",
       " 'log_level': 'info',\n",
       " 'use_tensorboard_logger': True,\n",
       " 'validation_split': 0.3,\n",
       " 'normalization_strategies': ['none'],\n",
       " 'max_runtime': 48000,\n",
       " 'min_budget': 300,\n",
       " 'max_budget': 24000,\n",
       " 'final_activation': 'sigmoid',\n",
       " 'loss_modules': ['bce_with_logits', 'bce_with_logits_weighted'],\n",
       " 'hyperparameter_search_space_updates': None,\n",
       " 'categorical_features': None,\n",
       " 'dataset_name': None,\n",
       " 'run_id': '0',\n",
       " 'task_id': -1,\n",
       " 'algorithm': 'bohb',\n",
       " 'portfolio_type': 'greedy',\n",
       " 'eta': 3,\n",
       " 'min_workers': 1,\n",
       " 'working_dir': '.',\n",
       " 'network_interface_name': 'eth0',\n",
       " 'memory_limit_mb': 1000000,\n",
       " 'run_worker_on_master_node': True,\n",
       " 'use_pynisher': True,\n",
       " 'refit_validation_split': 0.0,\n",
       " 'cross_validator': 'none',\n",
       " 'cross_validator_args': {},\n",
       " 'min_budget_for_cv': 0,\n",
       " 'shuffle': True,\n",
       " 'imputation_strategies': ['mean', 'median', 'most_frequent'],\n",
       " 'over_sampling_methods': ['none'],\n",
       " 'under_sampling_methods': ['none'],\n",
       " 'target_size_strategies': ['none'],\n",
       " 'initialization_methods': ['default', 'sparse'],\n",
       " 'initializer': 'simple_initializer',\n",
       " 'optimizer': ['adam', 'adamw', 'sgd', 'rmsprop'],\n",
       " 'additional_logs': [],\n",
       " 'optimize_metric': 'multilabel_accuracy',\n",
       " 'additional_metrics': [],\n",
       " 'batch_loss_computation_techniques': ['standard', 'mixup'],\n",
       " 'cuda': True,\n",
       " 'torch_num_threads': 1,\n",
       " 'full_eval_each_epoch': False,\n",
       " 'best_over_epochs': False,\n",
       " 'save_models': False,\n",
       " 'predict_model': None,\n",
       " 'early_stopping_patience': inf,\n",
       " 'early_stopping_reset_parameters': False,\n",
       " 'random_seed': 1510345191,\n",
       " 'num_iterations': 4}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autonet_config = {\n",
    "    \"result_logger_dir\" : \"logs/\",\n",
    "    \"budget_type\" : \"epochs\",\n",
    "    \"log_level\" : \"info\", \n",
    "    \"use_tensorboard_logger\" : True,\n",
    "    \"validation_split\" : 0.3,\n",
    "    'normalization_strategies': ['none'],\n",
    "    \"max_runtime\" : 48000,\n",
    "    \"min_budget\" : 300,\n",
    "    \"max_budget\" : 24000,\n",
    "    \"final_activation\" : 'sigmoid',\n",
    "    'networks': ['mlpnet', 'shapedmlpnet', 'resnet', 'shapedresnet'],\n",
    "    'loss_modules': ['bce_with_logits', 'bce_with_logits_weighted']\n",
    "    }\n",
    "autonet = AutoNetMultilabel(**autonet_config)\n",
    "autonet.get_current_autonet_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CreateDataLoader:batch_size': 361,\n",
       " 'Imputation:strategy': 'mean',\n",
       " 'InitializationSelector:initialization_method': 'default',\n",
       " 'InitializationSelector:initializer:initialize_bias': 'Yes',\n",
       " 'LearningrateSchedulerSelector:lr_scheduler': 'cosine_annealing',\n",
       " 'LossModuleSelector:loss_module': 'bce_with_logits',\n",
       " 'NetworkSelector:network': 'mlpnet',\n",
       " 'NormalizationStrategySelector:normalization_strategy': 'none',\n",
       " 'OptimizerSelector:optimizer': 'adamw',\n",
       " 'PreprocessorSelector:preprocessor': 'truncated_svd',\n",
       " 'ResamplingStrategySelector:over_sampling_method': 'none',\n",
       " 'ResamplingStrategySelector:target_size_strategy': 'none',\n",
       " 'ResamplingStrategySelector:under_sampling_method': 'none',\n",
       " 'TrainNode:batch_loss_computation_technique': 'standard',\n",
       " 'LearningrateSchedulerSelector:cosine_annealing:T_max': 289,\n",
       " 'LearningrateSchedulerSelector:cosine_annealing:eta_min': 1e-08,\n",
       " 'NetworkSelector:mlpnet:activation': 'sigmoid',\n",
       " 'NetworkSelector:mlpnet:num_layers': 8,\n",
       " 'NetworkSelector:mlpnet:num_units_1': 590,\n",
       " 'NetworkSelector:mlpnet:use_dropout': True,\n",
       " 'OptimizerSelector:adamw:learning_rate': 0.009838845880962514,\n",
       " 'OptimizerSelector:adamw:weight_decay': 0.020563725108234263,\n",
       " 'PreprocessorSelector:truncated_svd:target_dim': 82,\n",
       " 'NetworkSelector:mlpnet:dropout_1': 0.7010621721911834,\n",
       " 'NetworkSelector:mlpnet:dropout_2': 0.40382813484972885,\n",
       " 'NetworkSelector:mlpnet:dropout_3': 0.005566430867159866,\n",
       " 'NetworkSelector:mlpnet:dropout_4': 0.7908440713784841,\n",
       " 'NetworkSelector:mlpnet:dropout_5': 0.40001275424688454,\n",
       " 'NetworkSelector:mlpnet:dropout_6': 0.2296393678191726,\n",
       " 'NetworkSelector:mlpnet:dropout_7': 0.11567220522745761,\n",
       " 'NetworkSelector:mlpnet:dropout_8': 0.10015822063360283,\n",
       " 'NetworkSelector:mlpnet:num_units_2': 19,\n",
       " 'NetworkSelector:mlpnet:num_units_3': 57,\n",
       " 'NetworkSelector:mlpnet:num_units_4': 24,\n",
       " 'NetworkSelector:mlpnet:num_units_5': 669,\n",
       " 'NetworkSelector:mlpnet:num_units_6': 216,\n",
       " 'NetworkSelector:mlpnet:num_units_7': 16,\n",
       " 'NetworkSelector:mlpnet:num_units_8': 11}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample a random hyperparameter configuration to begin the \"refit\" process\n",
    "hyperparameter_config = autonet.get_hyperparameter_search_space().sample_configuration().get_dictionary()\n",
    "hyperparameter_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  \"please use `get_last_lr()`.\", UserWarning)\n",
      "/opt/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py:156: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit executed in 22830 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "results_fit = autonet.refit(X_train=X,\n",
    "                        Y_train=Y,\n",
    "                        hyperparameter_config=hyperparameter_config,\n",
    "                        autonet_config=autonet.get_current_autonet_config(),\n",
    "                        budget=24000)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"fit executed in %d seconds\" % int(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"logs/results_fit.json\", \"w\") as file:\n",
    "    json.dump(results_fit, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy score:  0.0008201202843083652\n"
     ]
    }
   ],
   "source": [
    "score = autonet.score(X_test=X_original, Y_test=Y_original)\n",
    "print(\"Model accuracy score: \", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = autonet.predict(X=X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00413671, 0.00318396, 0.00299999, ..., 0.00219377, 0.0033501 ,\n",
       "        0.00308354],\n",
       "       [0.00413671, 0.00318396, 0.00299999, ..., 0.00219377, 0.0033501 ,\n",
       "        0.00308354],\n",
       "       [0.00413671, 0.00318396, 0.00299999, ..., 0.00219377, 0.0033501 ,\n",
       "        0.00308354],\n",
       "       ...,\n",
       "       [0.00413671, 0.00318396, 0.00299999, ..., 0.00219377, 0.0033501 ,\n",
       "        0.00308354],\n",
       "       [0.00413671, 0.00318396, 0.00299999, ..., 0.00219377, 0.0033501 ,\n",
       "        0.00308354],\n",
       "       [0.00413671, 0.00318396, 0.00299999, ..., 0.00219377, 0.0033501 ,\n",
       "        0.00308354]], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [col for col in train_targets.columns]\n",
    "sample_submission[targets] = preds\n",
    "sample_submission.loc[test_features['cp_type']=='ctl_vehicle', targets] = 0\n",
    "sample_submission.to_csv('logs/preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041240330785512924"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max = 0\n",
    "for col in targets:\n",
    "    col_max = sample_submission[col].max()\n",
    "    max = max if max > col_max else col_max\n",
    "    \n",
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(\"logs/best_curr_kaggle_score.csv\")\n",
    "max = 0\n",
    "for col in targets:\n",
    "    col_max = test_df[col].max()\n",
    "    max = max if max > col_max else col_max\n",
    "    \n",
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
